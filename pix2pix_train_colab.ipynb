{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OE2gA774NFoq"
      },
      "source": [
        "# Training PIX2PIX models\n",
        "\n",
        "This notebook walks you through the steps of training your own machine learning model."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mmlXU0TONQLx",
        "outputId": "97ae2873-f65a-4aed-a890-12132bde73d7"
      },
      "outputs": [],
      "source": [
        "# Make sure you are connected to a runtime with a GPU\n",
        "!nvidia-smi -L"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OfOypxzWNBSy"
      },
      "outputs": [],
      "source": [
        "import tensorflow as tf\n",
        "\n",
        "import os\n",
        "import pathlib\n",
        "import time\n",
        "import datetime\n",
        "import glob\n",
        "import random\n",
        "import subprocess\n",
        "\n",
        "import numpy as np\n",
        "from matplotlib import pyplot as plt\n",
        "from IPython import display"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6XBfbblsNmAf"
      },
      "source": [
        "## Tweakable Parameters"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yRMcHhVJNYE_"
      },
      "outputs": [],
      "source": [
        "BUFFER_SIZE = 500\n",
        "BATCH_SIZE = 4\n",
        "IMAGE_WIDTH = 512\n",
        "IMAGE_HEIGHT = 512"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-k3UGkPTNoj_"
      },
      "outputs": [],
      "source": [
        "# Fix seeds\n",
        "SEED = 42 \n",
        "\n",
        "os.environ['PYTHONHASHSEED']=str(SEED)\n",
        "random.seed(SEED)\n",
        "np.random.seed(SEED)\n",
        "tf.random.set_seed(SEED)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IJB1nipyNtf3"
      },
      "source": [
        "## Load the dataset\n",
        "\n",
        "Here's where you come in! Let's upload our dataset:\n",
        "\n",
        "1. On your local machine, create a ZIP file of the directory.\n",
        "2. Open the \"Files\" sidebar to the left\n",
        "3. Drag the file (ending in `.zip`) to the sidebar.\n",
        "4. Wait for it to complete uploading before performing the next step."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "d-pjsTtROhfQ",
        "outputId": "859982de-d913-41e8-fec9-0ddf0b68610b"
      },
      "outputs": [],
      "source": [
        "# Optional: is the uploading taking too long? Use a preloaded file from [Pexels](https://www.pexels.com/video/woman-sitting-table-chef-4058070/) instead:\n",
        "# This file was already prepared in Figment\n",
        "!curl -O https://enigmeta.s3.amazonaws.com/2022-kikk-workshop/unsplash_woman_frames.zip\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8Mg0_PhZNwl3"
      },
      "outputs": [],
      "source": [
        "# Unzip the uploaded file\n",
        "!mkdir -p datasets/face\n",
        "!unzip -j -o -qq *.zip -d datasets/face "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wGDV2KXsPFS6"
      },
      "outputs": [],
      "source": [
        "# Some helper functions for creating/checking directories.\n",
        "\n",
        "def directory_should_exist(*args):\n",
        "    dir = os.path.join(*args)\n",
        "    if not os.path.isdir(dir):\n",
        "        raise Exception(\"Path '{}' is not a directory.\".format(dir))\n",
        "    return dir\n",
        "\n",
        "def ensure_directory(*args):\n",
        "    dir = os.path.join(*args)\n",
        "    try:\n",
        "        os.makedirs(dir)\n",
        "    except OSError as err:\n",
        "        if err.errno != 17:\n",
        "            raise err\n",
        "    return dir"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "n2XAXf5FPJ5C"
      },
      "outputs": [],
      "source": [
        "dataset_dir = directory_should_exist(\"datasets/face\")\n",
        "output_dir = ensure_directory(\"output\")\n",
        "checkpoint_dir = ensure_directory(output_dir, \"checkpoints\")\n",
        "log_dir = ensure_directory(output_dir, \"log\")\n",
        "output_images_dir = ensure_directory(output_dir, \"images\")\n",
        "glob_pattern = \"*.jpg\"\n",
        "image_files = glob.glob(os.path.join(dataset_dir, glob_pattern))\n",
        "assert len(image_files) > 0, \"No images found in directory '{}'\".format(dataset_dir)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 255
        },
        "id": "sioLoYa3PpSK",
        "outputId": "1064fc12-fe7e-4996-ae3a-0388fbc57d1d"
      },
      "outputs": [],
      "source": [
        "# Show an example image from the dataset\n",
        "sample_image_filename = random.choice(image_files)\n",
        "print(sample_image_filename)\n",
        "sample_image = tf.io.read_file(sample_image_filename)\n",
        "sample_image = tf.io.decode_jpeg(sample_image)\n",
        "print(sample_image.shape)\n",
        "plt.figure()\n",
        "plt.imshow(sample_image)\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3Fe_jprPQXXL"
      },
      "source": [
        "## Build an input pipeline\n",
        "All images need to go through some processing steps in order to prepare them for machine learning. Here we describe the steps."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jJM_FovXQR3t"
      },
      "outputs": [],
      "source": [
        "def load_image(fname):\n",
        "    # Read and decode an image file to a uint8 tensor\n",
        "    image = tf.io.read_file(fname)\n",
        "    image = tf.io.decode_jpeg(image)\n",
        "    \n",
        "    # Split the image tensor into two tensors:\n",
        "    # - The left (target) image\n",
        "    # - The right (source) image\n",
        "    image_width = tf.shape(image)[1]\n",
        "    image_width = image_width // 2\n",
        "    input_image = image[:, image_width:, :]\n",
        "    real_image = image[:, :image_width, :]\n",
        "    \n",
        "    # If needed you can flip the direction of the training here.\n",
        "    #real_image, input_image = input_image, real_image\n",
        "    \n",
        "    # Convert both images to float32 tensors\n",
        "    input_image = tf.cast(input_image, tf.float32)\n",
        "    real_image = tf.cast(real_image, tf.float32)\n",
        "        \n",
        "    return input_image, real_image"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_gjBXCLWQoOK"
      },
      "outputs": [],
      "source": [
        "def normalize_images(input_image, real_image):\n",
        "    # Normalize the images to [-1, 1]\n",
        "    input_image = (input_image / 127.5) - 1\n",
        "    real_image = (real_image / 127.5) - 1\n",
        "    return input_image, real_image"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AmO7QAJKQqHY"
      },
      "outputs": [],
      "source": [
        "def resize_image(input_image, real_image, width, height):\n",
        "    # Note that the order of width/height in tensorflow is reversed:\n",
        "    input_image = tf.image.resize(input_image, [height,width], method=tf.image.ResizeMethod.BICUBIC)\n",
        "    real_image = tf.image.resize(real_image, [height,width], method=tf.image.ResizeMethod.BICUBIC)\n",
        "    return input_image, real_image"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OcYytWZ8Qvm4"
      },
      "outputs": [],
      "source": [
        "def random_crop(input_image, real_image):\n",
        "    stacked_image = tf.stack([input_image, real_image], axis=0)\n",
        "    cropped_image = tf.image.random_crop(stacked_image, size=[2, IMAGE_HEIGHT, IMAGE_WIDTH, 3])\n",
        "    return cropped_image[0], cropped_image[1]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XOVe0y7OQwSA"
      },
      "outputs": [],
      "source": [
        "def random_jitter(input_image, real_image):\n",
        "    # Resize the image to 572 x 572\n",
        "    input_image, real_image = resize_image(input_image, real_image, IMAGE_WIDTH + 60, IMAGE_HEIGHT + 60)\n",
        "    # Randomly crop the image back to 512x512\n",
        "    input_image, real_image = random_crop(input_image, real_image)\n",
        "    # Randomly flip the image\n",
        "    if tf.random.uniform(()) > 0.5:\n",
        "        input_image = tf.image.flip_left_right(input_image)\n",
        "        real_image = tf.image.flip_left_right(real_image)\n",
        "    return input_image, real_image"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QW5YTsCPQxtI"
      },
      "outputs": [],
      "source": [
        "def process_image(fname):\n",
        "    input_image, real_image = load_image(fname)\n",
        "    input_image, real_image = normalize_images(input_image, real_image)\n",
        "    input_image, real_image = random_jitter(input_image, real_image)\n",
        "    return input_image, real_image"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 304
        },
        "id": "9bzn5knkQy44",
        "outputId": "0d056ccc-f4f5-4ec7-8c78-b628dc749745"
      },
      "outputs": [],
      "source": [
        "# Show an example image, processed through the data pipeline\n",
        "# Check that the segmented file is on the left, and the original file on the right.\n",
        "# If not, swap them around by commenting out the \"flip\" line in the load_image function\n",
        "# and run the above cells again.\n",
        "sample_input_image, sample_real_image = process_image(sample_image_filename)\n",
        "fig = plt.figure()\n",
        "plt.axis(\"off\")\n",
        "fig.add_subplot(1, 2, 1).imshow(sample_input_image * 0.5 + 0.5)\n",
        "fig.add_subplot(1, 2, 2).imshow(sample_real_image  * 0.5 + 0.5)\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dHVVs-ZVQ7Ao"
      },
      "outputs": [],
      "source": [
        "# Build the training dataset as a tf.data.Dataset object\n",
        "train_dataset = tf.data.Dataset.list_files(os.path.join(dataset_dir, glob_pattern))\n",
        "train_dataset = train_dataset.map(process_image, num_parallel_calls=tf.data.experimental.AUTOTUNE)\n",
        "train_dataset = train_dataset.shuffle(BUFFER_SIZE)\n",
        "train_dataset = train_dataset.batch(BATCH_SIZE)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 260
        },
        "id": "ZV4RftrbRJFo",
        "outputId": "862d69f7-befb-4624-8b5c-a0edeac5112a"
      },
      "outputs": [],
      "source": [
        "# Show an example processed image from the dataset\n",
        "sample_input_batch, sample_real_batch = list(train_dataset.take(1))[0]\n",
        "print(sample_input_batch.shape)\n",
        "fig = plt.figure()\n",
        "fig.add_subplot(1, 2, 1).imshow(sample_input_batch[0] * 0.5 + 0.5)\n",
        "fig.add_subplot(1, 2, 2).imshow(sample_real_batch[0]  * 0.5 + 0.5)\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gIOl1NV6RPYT"
      },
      "source": [
        "## Build the generator\n",
        "The generator is a modified U-Net, consisting of an enoder (downsampler) and decoder (upsampler).\n",
        "The encoder consists of a number of blocks. Each block contains convolution -> batch normalization -> leaky RELU\n",
        "The decoder consists of a number of blocks. Each block contains a transposed convolution -> batch normalization -> dropout (in first 3 blocks) -> ReLU\n",
        "There are skip connections between the encoder and decoder."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "W8eaMbtqRP4x"
      },
      "outputs": [],
      "source": [
        "def make_downsample_block(filters, size, apply_batch_norm=True):\n",
        "    initializer = tf.random_normal_initializer(mean=0.0, stddev=0.02)\n",
        "    block = tf.keras.Sequential()\n",
        "    block.add(tf.keras.layers.Conv2D(filters, size, strides=2, padding='same', kernel_initializer=initializer, use_bias=False))\n",
        "    #if apply_batch_norm:\n",
        "    #    block.add(tf.keras.layers.BatchNormalization())\n",
        "    block.add(tf.keras.layers.LeakyReLU())\n",
        "    return block"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yQPaFNxkRSxa"
      },
      "outputs": [],
      "source": [
        "def make_upsample_block(filters, size, apply_dropout=False):\n",
        "    initializer = tf.random_normal_initializer(mean=0.0, stddev=0.2)\n",
        "    block = tf.keras.Sequential()\n",
        "    block.add(tf.keras.layers.Conv2DTranspose(filters, size, strides=2, padding='same', kernel_initializer=initializer, use_bias=False))\n",
        "    block.add(tf.keras.layers.BatchNormalization())\n",
        "    #if apply_dropout:\n",
        "    #    block.add(tf.keras.layers.Dropout(0.5))\n",
        "    block.add(tf.keras.layers.ReLU())\n",
        "    return block"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kfYt20pbRTDw"
      },
      "outputs": [],
      "source": [
        "def Generator():\n",
        "    inputs = tf.keras.layers.Input(shape=[512, 512, 3])\n",
        "    down_stack = [\n",
        "        make_downsample_block(32,  4, apply_batch_norm=False), # (?, 256, 256,   32)\n",
        "        make_downsample_block(64,  4),                         # (?, 128, 128,   64)\n",
        "        make_downsample_block(128, 4),                         # (?,  64,  64,  128)\n",
        "        make_downsample_block(256, 4),                         # (?,  32,  32,  256)\n",
        "        make_downsample_block(512, 4),                         # (?,  16,  16,  512)\n",
        "        make_downsample_block(512, 4),                         # (?,   8,   8,  512)\n",
        "        make_downsample_block(512, 4),                         # (?,   4,   4,  512)\n",
        "        make_downsample_block(512, 4),                         # (?,   2,   2,  512)\n",
        "        make_downsample_block(512, 4),                         # (?,   1,   1,  512)\n",
        "    ]\n",
        "    \n",
        "    up_stack = [\n",
        "        make_upsample_block(512, 4, apply_dropout=True),       # (?,   2,   2, 1024)\n",
        "        make_upsample_block(512, 4, apply_dropout=True),       # (?,   4,   4, 1024)\n",
        "        make_upsample_block(512, 4, apply_dropout=True),       # (?,   8,   8, 1024)\n",
        "        make_upsample_block(512, 4),                           # (?,  16,  16, 1024)\n",
        "        make_upsample_block(256, 4),                           # (?,  32,  32,  512)\n",
        "        make_upsample_block(128, 4),                           # (?,  64,  64,  256)\n",
        "        make_upsample_block( 64, 4),                           # (?, 128, 128,  128)\n",
        "        make_upsample_block( 32, 4),                           # (?, 256, 256,   64)\n",
        "    ]    \n",
        "\n",
        "    initializer = tf.random_normal_initializer(mean=0.0, stddev=0.02)\n",
        "    # (?, 512, 512, 3)\n",
        "    last = tf.keras.layers.Conv2DTranspose(3, 4, strides=2, padding='same', kernel_initializer=initializer, activation='tanh')\n",
        "    \n",
        "    x = inputs\n",
        "    # Downsampling through the model\n",
        "    skips = []\n",
        "    for down in down_stack:\n",
        "        x = down(x)\n",
        "        skips.append(x)\n",
        "        \n",
        "    skips = reversed(skips[:-1])\n",
        "    \n",
        "    # Upsampling and establishing the skip connections\n",
        "    for up, skip in zip(up_stack, skips):\n",
        "        x = up(x)\n",
        "        x = tf.keras.layers.Concatenate()([x, skip])\n",
        "    \n",
        "    x = last(x)\n",
        "\n",
        "    return tf.keras.Model(inputs=inputs, outputs=x)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1blKVOx7RVEo"
      },
      "outputs": [],
      "source": [
        "# Instantiate and show the generator.\n",
        "generator = Generator()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 269
        },
        "id": "sYnxYPoTRhhy",
        "outputId": "167bb820-9b10-45b6-8d1d-c8fb0f93c9d6"
      },
      "outputs": [],
      "source": [
        "# Show the output of the generator. Note that this will be a random noise image, since we didn't do any training yet.\n",
        "gen_output = generator(sample_input_batch, training=False)\n",
        "plt.imshow(gen_output[0] * 0.5 + 0.5)\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "o70D1SGuR1uI"
      },
      "source": [
        "## Define the generator loss\n",
        "\n",
        "To evaluate the generator we need to calculate the loss; basically how far off it is from the original image."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1hKTDUQ-R01x"
      },
      "outputs": [],
      "source": [
        "LAMBDA = 100\n",
        "loss_object = tf.keras.losses.BinaryCrossentropy(from_logits=True)\n",
        "\n",
        "def generator_loss(disc_generated_output, gen_output, target):\n",
        "  gan_loss = loss_object(tf.ones_like(disc_generated_output), disc_generated_output)\n",
        "\n",
        "  # Mean absolute error\n",
        "  l1_loss = tf.reduce_mean(tf.abs(target - gen_output))\n",
        "\n",
        "  total_gen_loss = gan_loss + (LAMBDA * l1_loss)\n",
        "\n",
        "  return total_gen_loss, gan_loss, l1_loss"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ATsQEBKYR6-v"
      },
      "source": [
        "## Build the discriminator\n",
        "The discriminator is a convolutional PatchGAN classifier - it tries to classify if each image *patch* is real or not.\n",
        "- Each block in the discriminator is convolution -> batch normalization -> leaky ReLU.\n",
        "- The shape of the output after the last layer is (batch_size, 30, 30, 1).\n",
        "- Each 30 x 30 image patch of the output classifies a 70 x 70 portion of the input image\n",
        "- The discriminator receives two inputs:\n",
        "  - The input and the target image, which it should classify as real.\n",
        "  - The input image and the generated image, which it should classify as fake.\n",
        "  - Using `tf.concat([input, target], axis=-1)` we can concatenate these 2 inputs together."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XCKXX1M9Rnby"
      },
      "outputs": [],
      "source": [
        "def Discriminator():\n",
        "    initializer = tf.random_normal_initializer(mean=0.0, stddev=0.02)\n",
        "    input_image = tf.keras.layers.Input(shape=[512, 512, 3], name='input_image')\n",
        "    target_image = tf.keras.layers.Input(shape=[512, 512, 3], name='target_image')\n",
        "    x = tf.keras.layers.concatenate([input_image, target_image]) # (?, 512, 512,   6)\n",
        "    down1 = make_downsample_block( 32, 4, False)(x)              # (?, 256, 256,  32)\n",
        "    down2 = make_downsample_block( 64, 4)(down1)                 # (?, 128, 128,  64)\n",
        "    down3 = make_downsample_block(128, 4)(down2)                 # (?,  64,  64, 128)\n",
        "    down4 = make_downsample_block(256, 4)(down3)                 # (?,  32,  32, 256)\n",
        "    \n",
        "    zero_pad1 = tf.keras.layers.ZeroPadding2D()(down4)           # (?,  34,  34, 256)\n",
        "    conv = tf.keras.layers.Conv2D(512, 4, strides=1,\n",
        "                                  kernel_initializer=initializer,\n",
        "                                  use_bias=False)(zero_pad1)     # (?,  31,  31, 512)\n",
        "    batch_norm1 = tf.keras.layers.BatchNormalization()(conv)\n",
        "    leaky_relu = tf.keras.layers.LeakyReLU()(batch_norm1)\n",
        "    zero_pad2 = tf.keras.layers.ZeroPadding2D()(leaky_relu)      # (?,  33,  33, 512)\n",
        "    last = tf.keras.layers.Conv2D(1, 4, strides=1,\n",
        "                                  kernel_initializer=initializer)(zero_pad2) # (?, 30, 30, 1)\n",
        "    return tf.keras.Model(inputs=[input_image, target_image], outputs=last)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nfjLA70eRzGX"
      },
      "outputs": [],
      "source": [
        "# Instantiate the discriminator\n",
        "discriminator = Discriminator()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DOXzsxC4SBGS"
      },
      "source": [
        "## Define the discriminator loss\n",
        "The discriminator loss takes real images and generated images.\n",
        "- `real_loss` is a sigmoid cross-entropy loss of the real images and the array of ones.\n",
        "- `generated_loss` is a sigmoid cross-entropy loss of the generated images and the arrays of zeros.\n",
        "- `total_loss` is the sum of the two."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "q7QGTyBAR-6B"
      },
      "outputs": [],
      "source": [
        "def discriminator_loss(disc_real_output, disc_generated_output):\n",
        "    real_loss = loss_object(tf.ones_like(disc_real_output), disc_real_output)\n",
        "    generated_loss = loss_object(tf.zeros_like(disc_generated_output), disc_generated_output)\n",
        "    total_loss = real_loss + generated_loss\n",
        "    return total_loss"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iBZ9VwXESFnk"
      },
      "source": [
        "# Define the optimizers and checkpoint saver"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bNYOUrgQSCtB"
      },
      "outputs": [],
      "source": [
        "generator_optimizer = tf.keras.optimizers.Adam(2e-4, beta_1=0.5)\n",
        "discriminator_optimizer = tf.keras.optimizers.Adam(2e-4, beta_1=0.5)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "I1kACsfPSG32"
      },
      "outputs": [],
      "source": [
        "checkpoint_prefix = os.path.join(checkpoint_dir, 'ckpt')\n",
        "checkpoint = tf.train.Checkpoint(generator_optimizer=generator_optimizer,\n",
        "                                 discriminator_optimizer=discriminator_optimizer,\n",
        "                                 generator=generator,\n",
        "                                 discriminator=discriminator)\n",
        "checkpoint_manager = tf.train.CheckpointManager(\n",
        "    checkpoint, checkpoint_dir, max_to_keep=5\n",
        ")\n",
        "checkpoint_manager.restore_or_initialize()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Mbk0FIduSKeX"
      },
      "source": [
        "## Generate images\n",
        "Define a function that generates images during training."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rJo2deBdSHoh"
      },
      "outputs": [],
      "source": [
        "def save_image(filename, tensor):\n",
        "    normalized = tensor * 0.5 + 0.5\n",
        "    image_tensor = tf.cast(normalized * 255, tf.uint8)\n",
        "    tf.io.write_file(os.path.join(output_images_dir, filename), tf.io.encode_jpeg(image_tensor))\n",
        "\n",
        "def generate_images(model, test_input, target, save=False, step=1):\n",
        "    prediction = model(test_input, training=False)\n",
        "    \n",
        "    plt.figure(figsize=(15, 15))\n",
        "    display_list = [test_input[0], target[0], prediction[0]]\n",
        "    title = ['Input Image', 'Real Image', 'Predicted Image']\n",
        "    for i in range(3):\n",
        "        plt.subplot(1, 3, i+1)\n",
        "        plt.title(title[i])\n",
        "        plt.imshow(display_list[i] * 0.5 + 0.5)\n",
        "        plt.axis('off')\n",
        "    plt.show()\n",
        "\n",
        "    if save:\n",
        "        if step == 0:\n",
        "            save_image(f\"input-{step:05d}.jpg\", test_input[0])\n",
        "            save_image(f\"target-{step:05d}.jpg\", target[0])\n",
        "        save_image(f'output-{step:05d}.jpg', prediction[0])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 331
        },
        "id": "radMnAKhSMKh",
        "outputId": "219b834d-a069-46c7-e950-665efba93577"
      },
      "outputs": [],
      "source": [
        "# Test image generation\n",
        "# Note that the predicted image will be random noise, since we haven't done any training yet!\n",
        "for example_input, example_target in train_dataset.take(1):\n",
        "    generate_images(generator, example_input, example_target)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "se3oKFqQSV4G"
      },
      "source": [
        "# Training loop"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jwIxR8nKSSVB"
      },
      "outputs": [],
      "source": [
        "summary_writer = tf.summary.create_file_writer(log_dir + \"/fit/\" + datetime.datetime.now().strftime('%Y%m%d-%H%M%S'))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tc5y1EkbSYPh"
      },
      "outputs": [],
      "source": [
        "@tf.function\n",
        "def train_step(input_image, target, step):\n",
        "    with tf.GradientTape() as gen_tape, tf.GradientTape() as disc_tape:\n",
        "        gen_output = generator(input_image, training=True)\n",
        "        disc_real_output = discriminator([input_image, target], training=True)\n",
        "        disc_generated_output = discriminator([input_image, gen_output], training=True)\n",
        "        \n",
        "        gen_total_loss, gen_gan_loss, gen_l1_loss = generator_loss(disc_generated_output, gen_output, target)\n",
        "        disc_loss = discriminator_loss(disc_real_output, disc_generated_output)\n",
        "    generator_gradients = gen_tape.gradient(gen_total_loss, generator.trainable_variables)\n",
        "    discriminator_gradients = disc_tape.gradient(disc_loss, discriminator.trainable_variables)\n",
        "    generator_optimizer.apply_gradients(zip(generator_gradients, generator.trainable_variables))\n",
        "    discriminator_optimizer.apply_gradients(zip(discriminator_gradients, discriminator.trainable_variables))\n",
        "    \n",
        "    with summary_writer.as_default():\n",
        "        tf.summary.scalar('gen_total_loss', gen_total_loss, step=step//1000)\n",
        "        tf.summary.scalar('gen_gan_loss', gen_gan_loss, step=step//1000)\n",
        "        tf.summary.scalar('gen_l1_loss', gen_l1_loss, step=step//1000)\n",
        "        tf.summary.scalar('disc_loss', disc_loss, step=step//1000)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IB0AYWnESaE_"
      },
      "outputs": [],
      "source": [
        "def fit(train_ds, steps):\n",
        "    example_input, example_target = next(iter(train_ds.take(1)))\n",
        "    start = time.time()\n",
        "    \n",
        "    for step, (input_image, target) in train_ds.repeat().take(steps).enumerate():\n",
        "        if step % 1000 == 0:\n",
        "            display.clear_output(wait=True)\n",
        "            if step != 0:\n",
        "                print(f'Time taken for 1000 steps: {time.time()-start:.2f} sec\\n')\n",
        "            start = time.time()\n",
        "            generate_images(generator, example_input, example_target, save=True, step=step//1000)\n",
        "            print(f'Step: {step//1000}k')\n",
        "\n",
        "        train_step(input_image, target, step)\n",
        "        \n",
        "        if (step + 1) % 10 == 0:\n",
        "            print('.', end='', flush=True)\n",
        "            \n",
        "        if (step + 1) % 20000 == 0:\n",
        "            checkpoint.save(file_prefix=checkpoint_prefix)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 91
        },
        "id": "EL5zAGbqSacp",
        "outputId": "ef7ddbae-f063-41af-b39d-6e4d98af7efc"
      },
      "outputs": [],
      "source": [
        "fit(train_dataset, steps=50_000)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GS2osOpmSexh"
      },
      "outputs": [],
      "source": [
        "# After training is complete, save the checkpoint\n",
        "checkpoint.save(file_prefix=checkpoint_prefix)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pBqmXAyeShOh"
      },
      "outputs": [],
      "source": [
        "# Save the generator\n",
        "# We need this when converting it to tensorflowjs\n",
        "generator_dir = os.path.join(output_dir, 'generator')\n",
        "generator.save(generator_dir)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YbDIVxCN4RpN"
      },
      "source": [
        "## Convert to Tensorflow.js\n",
        "In order to run the model in realtime in Figment, we need to convert it to tensorflow.js. \n",
        "\n",
        "We'll install the library first, then run the appropriate commands. The output will be a ZIP file in the `project` directory."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3FGFdxjd2Mdv"
      },
      "outputs": [],
      "source": [
        "%pip install -q tensorflowjs"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "s4rsnV3K3KlR"
      },
      "outputs": [],
      "source": [
        "import subprocess\n",
        "import shutil\n",
        "tfjs_dir = os.path.join(output_dir, 'tfjs')\n",
        "subprocess.run(['tensorflowjs_converter', generator_dir, tfjs_dir])\n",
        "shutil.make_archive(tfjs_dir, 'zip', output_dir, 'tfjs')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wPo89KeN4d3Y"
      },
      "source": [
        "Finished! Now go download the `tfjs.zip` file in the `project` directory (click the three dots and choose \"Download\")."
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "collapsed_sections": [],
      "provenance": []
    },
    "gpuClass": "premium",
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
